# â˜¸ï¸ Kubernetes Lab: Autoscaling (HPA) & Stress Testing

![Kubernetes](https://img.shields.io/badge/Kubernetes-1.29-blue?logo=kubernetes)
![Vagrant](https://img.shields.io/badge/Vagrant-VirtualLab-1868F2?logo=vagrant)
![VirtualBox](https://img.shields.io/badge/VirtualBox-Hypervisor-183A61?logo=virtualbox)

> LaboratÃ³rio prÃ¡tico para demonstrar **Horizontal Pod Autoscaling (HPA)** em Kubernetes sob **estresse de memÃ³ria**, utilizando ambiente **On-Premise virtualizado**.

Este projeto implementa um cluster Kubernetes **On-Premise** virtualizado utilizando **Vagrant** e **VirtualBox**. O objetivo Ã© demonstrar, na prÃ¡tica, o funcionamento do **Horizontal Pod Autoscaling (HPA)** sob condiÃ§Ãµes de estresse de memÃ³ria.

## ğŸ“‘ Ãndice

* [VisÃ£o Geral](#-kubernetes-lab-autoscaling-hpa--stress-testing)
* [Arquitetura e Topologia](#-1-arquitetura-e-topologia)
* [Tecnologias Utilizadas](#stack-utilizada)
* [InstalaÃ§Ã£o e ExecuÃ§Ã£o](#-2-guia-de-instalaÃ§Ã£o-e-execuÃ§Ã£o)
* [PreparaÃ§Ã£o do Teste](#-3-preparaÃ§Ã£o-do-teste-master)
* [Teste de Estresse](#-4-execuÃ§Ã£o-do-teste-de-estresse)
* [Resultados Esperados](#-resultado-esperado)


---

## ğŸ— 1. Arquitetura e Topologia

O ambiente Ã© composto por **3 MÃ¡quinas Virtuais** rodando **Ubuntu Server 20.04 LTS**:

| Hostname         | FunÃ§Ã£o        | Specs (Lab)       | IP (Bridge)   |
| :--------------- | :------------ | :---------------- | :------------ |
| **k8s-master**   | Control Plane | 2 vCPU, 1.5GB RAM | 192.168.56.10 |
| **k8s-worker-1** | Worker Node   | 1 vCPU, 1.0GB RAM | 192.168.56.11 |
| **k8s-worker-2** | Worker Node   | 1 vCPU, 1.0GB RAM | 192.168.56.12 |

**Stack utilizada:**

* â˜¸ï¸ Kubernetes v1.29 (kubeadm)
* ğŸ“¦ Container Runtime: Containerd
* ğŸ“Š Monitoramento: Metrics Server + K9s
* ğŸŒ CNI: Calico

---

## ğŸš€ 2. Guia de InstalaÃ§Ã£o e ExecuÃ§Ã£o

### 2.1. Provisionamento do Ambiente

```bash
# Clone o repositÃ³rio
git clone https://github.com/Wellington126/kubernetes-autoscaling-lab.git
cd lab-kubernetes

# Inicie as mÃ¡quinas virtuais
vagrant up
```

---

### 2.2. InstalaÃ§Ã£o do Kubernetes (Todos os NÃ³s)

Acesse cada VM via `vagrant ssh` e execute como **root** (`sudo -i`).

```bash
# === 1. ConfiguraÃ§Ãµes de Sistema ===
swapoff -a
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

cat <<EOF | tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

modprobe overlay && modprobe br_netfilter

cat <<EOF | tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl --system

# === 2. Instalar Containerd ===
apt-get update && apt-get install -y containerd apt-transport-https ca-certificates curl gpg
mkdir -p /etc/containerd
containerd config default | tee /etc/containerd/config.toml
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
systemctl restart containerd

# === 3. Instalar Kubeadm, Kubelet e Kubectl (v1.29) ===
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' \
| tee /etc/apt/sources.list.d/kubernetes.list

apt-get update && apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl

# === 4. CorreÃ§Ã£o de Rede (Vagrant) ===
IP_ADDR=$(ip -4 addr show eth1 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')
echo "KUBELET_EXTRA_ARGS='--node-ip=${IP_ADDR}'" > /etc/default/kubelet

systemctl daemon-reload && systemctl restart kubelet
```

---

### 2.3. InicializaÃ§Ã£o do Cluster (Apenas no Master)

```bash
# Inicializar o Control Plane
kubeadm init \
  --apiserver-advertise-address=192.168.56.10 \
  --pod-network-cidr=192.168.0.0/16

# Configurar kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Instalar Calico
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml
```

â¡ï¸ ApÃ³s isso, execute o comando `kubeadm join` nos **Workers**.

---

## âš™ï¸ 3. PreparaÃ§Ã£o do Teste (Master)

### 3.1. Metrics Server

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

kubectl patch deployment metrics-server -n kube-system \
  --type='json' \
  -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'
```

---

### 3.2. K9s (Monitoramento via Terminal)

```bash
curl -sS https://webinstall.dev/k9s | bash
source ~/.config/envman/PATH.env
```

---

### 3.3. Deploy da AplicaÃ§Ã£o e do HPA

```bash
kubectl apply -f stress.yaml
kubectl apply -f hpa.yaml
```

---

## ğŸ§ª 4. ExecuÃ§Ã£o do Teste de Estresse

O teste injeta carga de memÃ³ria **acima do limite configurado no HPA (64MB)**, forÃ§ando o escalonamento automÃ¡tico.

### Passo 1: Monitorar

```bash
k9s
```

### Passo 2: Gerar Estresse

```bash
kubectl exec -it \
$(kubectl get pod -l run=php-apache -o jsonpath="{.items[0].metadata.name}") \
-- stress-ng --vm 1 --vm-bytes 200M --timeout 600s
```

ğŸ“ˆ Durante o teste, observe:

* Aumento do consumo de memÃ³ria
* CriaÃ§Ã£o automÃ¡tica de novos Pods
* Balanceamento da carga entre os Workers

---

## ğŸ“Š Resultado Esperado

* O **HPA detecta o uso excessivo de memÃ³ria**
* Novos Pods sÃ£o criados automaticamente
* O cluster mantÃ©m estabilidade mesmo sob estresse

---
ğŸ“Œ **LaboratÃ³rio ideal para estudos de:**

* Autoscaling
* Observabilidade
* Kubernetes On-Premise
* Testes de desempenho
